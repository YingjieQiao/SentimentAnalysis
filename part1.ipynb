{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import sys\n",
    "# print(sys.path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "ES_TRAINING_DATA = \"./ES/train\"\n",
    "ES_TESTING_DATA = \"./ES/dev.in\"\n",
    "ES_LABELED_DATA = \"./ES/dev.out\"\n",
    "\n",
    "RU_TRAINING_DATA = \"./RU/train\"\n",
    "RU_TESTING_DATA = \"./RU/dev.in\"\n",
    "RU_LABELED_DATA = \"./RU/dev.out\"\n",
    "\n",
    "def readTrainingData(filePath):\n",
    "    data = []\n",
    "    with open(filePath) as file:\n",
    "        data = file.read().splitlines()\n",
    "    \n",
    "    output = []\n",
    "    for line in data:\n",
    "        line = line.split(\" \")\n",
    "        if(len(line) > 2):\n",
    "            # . â€¦ O\n",
    "            line = [\" \".join(line[0:len(line)-1]), line[len(line)-1]]\n",
    "            output.append(line)\n",
    "        else:\n",
    "            output.append(line)\n",
    "        \n",
    "    return output\n",
    "\n",
    "\n",
    "def readTestData(filePath):\n",
    "    data = []\n",
    "    with open(filePath) as file:\n",
    "        data = file.read().splitlines()\n",
    "    \n",
    "    output = []\n",
    "    for line in data:\n",
    "        output.append(line)\n",
    "    \n",
    "    return output\n",
    "\n",
    "es_training_data = readTrainingData(ES_TRAINING_DATA)\n",
    "ru_training_data = readTrainingData(RU_TRAINING_DATA)\n",
    "es_testing_data = readTestData(ES_TESTING_DATA)\n",
    "ru_testing_data = readTestData(RU_TESTING_DATA)\n",
    "es_labeled_data = readTrainingData(ES_LABELED_DATA)\n",
    "ru_labeled_data = readTrainingData(RU_LABELED_DATA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Write a function that estimates the emission parameters from the training set using MLE (maximum\n",
    "likelihood estimation)\n",
    "\n",
    "One problem with estimating the emission parameters is that some words that appear in the test set\n",
    "do not appear in the training set. One simple idea to handle this issue is as follows. We introduce\n",
    "a special word token #UNK#, and make the following modifications to the computation of emission\n",
    "probabilitie\n",
    "\"\"\"\n",
    "\n",
    "def emissionMLE(dataset, k=1):\n",
    "    observations = {} \n",
    "    tags = {} # tag/state\n",
    "    tagToObs = {}\n",
    "\n",
    "    for data in dataset:\n",
    "        if len(data) < 2:\n",
    "            continue\n",
    "        o = data[0]\n",
    "        t = data[1]\n",
    "\n",
    "        if o not in observations:\n",
    "            observations[o] = 0\n",
    "        observations[o] += 1\n",
    "\n",
    "        if t not in tags:\n",
    "            tags[t] = 0\n",
    "        tags[t] += 1\n",
    "\n",
    "        key = (t, o)\n",
    "        if key not in tagToObs:\n",
    "            tagToObs[key] = 0\n",
    "        tagToObs[key] += 1\n",
    "\n",
    "    res = {}\n",
    "    for key in tagToObs.keys():\n",
    "        res[key] = tagToObs[key] / (tags[key[0]] + k)\n",
    "    for key in tags.keys():\n",
    "        unk_key = (key, \"#UNK#\")\n",
    "        res[unk_key] = k / (tags[key] + k)\n",
    "    \n",
    "    return res\n",
    "\n",
    "es_emission = emissionMLE(dataset=es_training_data, k=1)\n",
    "ru_emission = emissionMLE(dataset=ru_training_data, k=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "#Entity in gold data: 255\n",
      "#Entity in prediction: 1288\n",
      "\n",
      "#Correct Entity : 184\n",
      "Entity  precision: 0.1429\n",
      "Entity  recall: 0.7216\n",
      "Entity  F: 0.2385\n",
      "\n",
      "#Correct Sentiment : 109\n",
      "Sentiment  precision: 0.0846\n",
      "Sentiment  recall: 0.4275\n",
      "Sentiment  F: 0.1413\n",
      "\n",
      "#Entity in gold data: 461\n",
      "#Entity in prediction: 1377\n",
      "\n",
      "#Correct Entity : 301\n",
      "Entity  precision: 0.2186\n",
      "Entity  recall: 0.6529\n",
      "Entity  F: 0.3275\n",
      "\n",
      "#Correct Sentiment : 131\n",
      "Sentiment  precision: 0.0951\n",
      "Sentiment  recall: 0.2842\n",
      "Sentiment  F: 0.1425\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Implement a simple sentiment analysis system that produces the tag for each word x in the sequence\n",
    "\"\"\"\n",
    "\n",
    "def argmax(emission):\n",
    "    map = {}\n",
    "    res = {}\n",
    "    for key, value in emission.items():\n",
    "        t = key[0]\n",
    "        o = key[1]\n",
    "\n",
    "        if o not in map:\n",
    "            map[o] = value\n",
    "            res[o] = t\n",
    "        else:\n",
    "            if value > map[o]:\n",
    "                map[o] = value\n",
    "                res[o] = t\n",
    "\n",
    "    return res\n",
    "\n",
    "es_seqmap = argmax(es_emission)\n",
    "ru_seqmap = argmax(ru_emission)\n",
    "\n",
    "ES_PRED = \"./ES/dev.p1.out\"\n",
    "RU_PRED = \"./RU/dev.p1.out\"\n",
    "\n",
    "def serializeSeqmapAndWriteToFile(seqmap, testData, predFilePath):\n",
    "    serializedSeqmap = []\n",
    "    with open(predFilePath, \"w\") as file:\n",
    "        for data in testData:\n",
    "            if data is None or len(data) == 0:\n",
    "                file.write(\"\\n\")\n",
    "                continue\n",
    "            entry = [data]\n",
    "            if data not in seqmap:\n",
    "                entry.append(\"#UNK#\")\n",
    "            else:\n",
    "                entry.append(seqmap[data])\n",
    "            serializedSeqmap.append(entry)\n",
    "            file.write(\"{} {}\\n\".format(entry[0], entry[1]))\n",
    "\n",
    "    return serializedSeqmap\n",
    "        \n",
    "es_serialized_seqmap = serializeSeqmapAndWriteToFile(es_seqmap, es_testing_data, ES_PRED)\n",
    "ru_serialized_seqmap = serializeSeqmapAndWriteToFile(ru_seqmap, ru_testing_data, RU_PRED)\n",
    "\n",
    "# print(len(es_testing_data))\n",
    "# print(len(es_testing_data[0]))\n",
    "# print(len(es_seialized_seqmap))\n",
    "# print(len(es_seialized_seqmap[0]))\n",
    "# print(len(es_labeled_data))\n",
    "# print(len(es_labeled_data[0]))\n",
    "\n",
    "\n",
    "def calculateScore(LABELED_DATA, PREDICTED_DATA):\n",
    "    os.system(\"python3 EvalScript/evalResult.py {} {}\".format(LABELED_DATA, PREDICTED_DATA))\n",
    "\n",
    "\n",
    "calculateScore(ES_LABELED_DATA, ES_PRED)\n",
    "calculateScore(RU_LABELED_DATA, RU_PRED)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "e485bb5d8f1d8ef2575d7ba8353c73a1ec82aa3e63a21c343a17a733c833a62e"
  },
  "kernelspec": {
   "display_name": "Python 3.8.1 64-bit ('venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
